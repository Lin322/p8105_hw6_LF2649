---
title: "Homework 6"
author: "Lin Feng"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


### Problem 0



```{r load_libraries}
library(modelr)
library(p8105.datasets)
```

### Problem 1

* Create a city_state variable (e.g. “Baltimore, MD”), 
* and a binary variable indicating whether the homicide is solved. 
* Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t report victim race. Also omit Tulsa, AL – this is a data entry mistake. 
* For this problem, limit your analysis those for whom victim_race is white or black. 
* Be sure that victim_age is numeric

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) %>% 
  filter(
    victim_race %in% c("White", "Black"),
    city_state != "Tulsa, AL") %>% 
  select(city_state, resolution, victim_age, victim_race, victim_sex)

homicide_df
```


Start with one city.

*For the city of Baltimore, MD, use the glm function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. 
* Save the output of glm as an R object; apply the broom::tidy to this object; and obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing non-white victims to white victims keeping all other variables fixed.

```{r}
baltimore_df =
  homicide_df %>% 
  filter(city_state == "Baltimore, MD")
glm(resolution ~ victim_age + victim_race + victim_sex, 
    data = baltimore_df,
    family = binomial()) %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(term, OR, starts_with("CI")) %>% 
  knitr::kable(digits = 3)
```


Try this across cities.
* Now run glm for each of the cities in your dataset, 
* and extract the adjusted odds ratio (and CI) for solving homicides comparing Black victims to white victims. 
* Do this within a “tidy” pipeline, making use of purrr::map, list columns, and unnest as necessary to create a dataframe with estimated ORs and CIs for each city.

```{r}
models_results_df = 
  homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = 
      map(.x = data, ~glm(resolution ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())),
    results = map(models, broom::tidy)
  ) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(city_state, term, OR, starts_with("CI")) 
```

* Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot.
```{r}
models_results_df %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```



## Problem 2

Find some residuals
* Load and clean the data for regression analysis (i.e. convert numeric to factor where appropriate, check for missing data, etc.).

```{r}
baby_df = 
  read_csv("./data/birthweight.csv") %>% 
  mutate(
    frace = as.factor(frace),
    mrace = as.factor(mrace)
  ) %>% 
  drop_na()

baby_df
```

### fit a model using delwt, bhead, gaweeks, and momage as predictors

* Propose a regression model for birthweight. 
* This model may be based on a hypothesized structure for the factors that underly birthweight, on a data-driven model-building process, or a combination of the two. 
* Describe your modeling process and show a plot of model residuals against fitted values – use add_predictions and add_residuals in making this plot.

```{r}
model_fit = lm(bwt ~ delwt + bhead + gaweeks + momage, data = baby_df)
```

### making a plot
```{r}
baby_df %>% 
  modelr::add_residuals(model_fit) %>% 
  modelr::add_predictions(model_fit) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point()
```


####Answer: 
* The predictors choice and model building are based on previous knowledge of the possible factors that can affect baby birthweight. 

### Compare model_fit to two others:

* One using length at birth and gestational age as predictors (main effects only)
* One using head circumference, length, sex, and all interactions (including the three-way interaction) between these
* Make this comparison in terms of the cross-validated prediction error; use crossv_mc and functions in purrr as appropriate.

```{r}
#build the two other models
model_main = lm(bwt ~ blength + gaweeks, data = baby_df)
model_intrct = lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = baby_df)

#compare models
cv_df = 
  crossv_mc(baby_df, 100)

cv_df %>% pull(train) %>% .[[1]] %>% as_tibble
cv_df %>% pull(test) %>% .[[1]] %>% as_tibble

cv_df =
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df %>% 
  mutate(
    model_fit  = map(train, ~lm(bwt ~ delwt + bhead + gaweeks + momage, data = .x)),
    model_main  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    model_intrct  = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = .x))) %>% 
  mutate(
    rmse_fit = map2_dbl(model_fit, test, ~rmse(model = .x, data = .y)),
    rmse_main = map2_dbl(model_main, test, ~rmse(model = .x, data = .y)),
    rmse_intrct = map2_dbl(model_intrct, test, ~rmse(model = .x, data = .y)))

#Compare model by RMSE 
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()


```
####Answer: 
* From the plot we can find that the interaction model has the least rmse among the three. 


## Problem 3
* For this problem, we’ll use the 2017 Central Park weather data that we’ve seen elsewhere. 
* The code chunk below (adapted from the course website) will download these data.
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

* We’ll focus on a simple linear regression with tmax as the response and tmin as the predictor, and are interested in the distribution of two quantities estimated from these data:
* Note: broom::glance() is helpful for extracting r^2 from a fitted regression, and broom::tidy() (with some additional wrangling) should help in computing log(β^0∗β^1)

### fit the log_beta calculation
```{r}
log_beta = 
  weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
   select(term, estimate) %>%
  mutate(
    term = str_replace(term,"\\(Intercept\\)","Intercept_beta"),
    term = str_replace(term,"tmin","tmin_beta"),
    ) %>%
  pivot_wider(
    names_from = "term",
    values_from = "estimate"
    ) %>% 
   unnest(Intercept_beta, tmin_beta) %>% 
   mutate(log = log(Intercept_beta) + log(tmin_beta)) #Since log(β^0∗β^1) = log(beta_0) + log(beta_1)
  
log_plot = 
  log_beta %>% 
  ggplot(aes(x = log)) + geom_density()
  
#95%CI
log_beta %>% 
  mutate(
    ci_lower = quantile(log, 0.025), 
    ci_upper = quantile(log, 0.975)
  ) %>% 
  select(ci_lower, ci_upper) %>% 
  head(1)



```


### fit the r_2 calculation
```{r}
r_2 = 
weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  mutate(r_square = r.squared) %>% 
  select(r_square) 



r_2_plot = 
  r_2 %>% 
  ggplot(aes(x = r_square)) + geom_density()
  


#95%CI
r_2 %>% 
  mutate(
    ci_lower = quantile(r_square, 0.025), 
    ci_upper = quantile(r_square, 0.975)
  ) %>% 
  select(ci_lower, ci_upper) %>% 
  head(1)

  
```


